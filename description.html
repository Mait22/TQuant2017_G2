<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




</head>

<body>

<style type="text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="fluid-row" id="header">




</div>


<div id="what-we-did" class="section level3">
<h3><span style="color: #84BD93">What we did </span></h3>
<p>Anlogous to the paper, we first set initial values for the parameter of the probability functions: <span class="math display">\[
M1: \pi_{i} = (1 + t_{i})^{-a}\\
M2: \pi_i = (b + t_{i})^{-a}\\
M3: \pi_i = (1 + b*t_{i})^{-a}
\]</span> Then data ist generated using a binomial distribution with <span class="math inline">\(n = 50\)</span>, and using the pi from the respective model, as well as <span class="math inline">\(t = (.1, 2.1, 4.1, 6.1, 8.1)\)</span>. With this data, maxmimum likelihood estimations were produced using the likelihood <span class="math display">\[L: \binom{n}{y_{i}} * \pi_{i}^{y_{i}} * (1 - \pi_{i})^{n - y_{i}}. \]</span></p>
<p>These estimation then were used as a basis for addition data generation. A sampling error was introduced. In this case we used a proportion of the real probability as well as a complementary proportion of a uniform distribution of values between 0 and 1.</p>
<p>Having thus generated data from each model, each model then was fitted to this generated data. For each fit GoF indices were computed (RMSE, PVAF) as well as the AIC <span class="math inline">\((-2(loglike) + 2k)\)</span> and BIC <span class="math inline">\((-2(loglike) + k(ln(n)))\)</span>. This step was repeated N times, and the proportion of choosing the respective model given a indice was then computed for each dataset.</p>
<p>Given the results from the paper, RMSE and PVAf should prove an ineffective way in which the proper generator model will be selected, because they will always go for the model with the best fit towards the data. Using AIC, or BIC for example, should allow the user to not only fit the observed data, but also select the model which might predict future data better.</p>
</div>
<div id="glossary-from-myung-2001" class="section level3">
<h3><span style="color: #84BD93"> Glossary, from Myung, 2001</span></h3>
<ul>
<li><p>Complexity: the property of a model that enables it to fit diverse patterns of data; it is the flexibility of a model. Although the number of parameters in a model and its functional form can be useful for gauging its complexity, a more accurate and intuitive measure is the number of distinct probability distributions that the model can generate by varying its parameters over their entire range.</p></li>
<li><p>Functional form: the way in which the parameters <span class="math inline">\((\beta)\)</span> and data (x) are combined in a model’s equation: <span class="math inline">\(y = \beta x\)</span> and <span class="math inline">\(y = \beta + x\)</span> have the same number of parameters but different functional forms (multiplicative versus additive).</p></li>
<li><p>Generalizability: the ability of a model to fit all data samples generated by the same cognitive process, not just the currently observed sample (i.e. the model’s expected GOF with respect to new data samples). Generalizability is estimated by combining a model’s GOF with a measure of its complexity.</p></li>
<li><p>Goodness of fit (GOF): the precision with which a model fits a particular sample of observed data. The predictions of the model are compared with the observed data. The discrepancy between the two is measured in a number of ways, such as calculating the root mean squared error between them.</p></li>
<li><p>Overfitting: the case where, in addition to fitting the main trends in the data, a model also fits the microvariation from this main trend at each data point.</p></li>
<li><p>Parameters: variables in a model’s equation that represent mental constructs or processes; they are adjusted to improve a model’s fit to data. For example, in the model <span class="math inline">\(y = \beta x\)</span>, <span class="math inline">\(\beta\)</span> is a parameter.</p></li>
</ul>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
